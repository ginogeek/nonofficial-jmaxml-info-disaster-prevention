import streamlit as st
import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
import pandas as pd
import io

KISHOU_XML_PAGE_URL = "https://www.data.jma.go.jp/developer/xml/feed/extra_l.xml"

st.set_page_config(page_title="ã€Œæ°—è±¡ç‰¹åˆ¥è­¦å ±ãƒ»è­¦å ±ãƒ»æ³¨æ„å ±ã€ç™ºè¡¨å±¥æ­´æ¤œç´¢ãƒ„ãƒ¼ãƒ« from æ°—è±¡åº é˜²ç½æƒ…å ±XMLï¼ˆé•·æœŸãƒ•ã‚£ãƒ¼ãƒ‰ï¼‰", layout="wide")

# --- èª¬æ˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæ°—è±¡åºé˜²ç½æƒ…å ±XMLã®æ¦‚è¦ï¼‰ ---
with st.expander("ğŸ“˜ æ°—è±¡åºé˜²ç½æƒ…å ±XMLã¨ã¯ï¼Ÿ", expanded=True):
    st.markdown("""
    ## é˜²ç½æƒ…å ±XMLã¨ã¯
    - æ°—è±¡åºã¯ã€æ°—è±¡ãƒ»æ´¥æ³¢ãƒ»åœ°éœ‡ãƒ»ç«å±±ãªã©ã®é˜²ç½æƒ…å ±ã‚’è¿…é€Ÿã‹ã¤æ­£ç¢ºã«ä¼ãˆã‚‹ãŸã‚ã«
    ã€Œæ°—è±¡åºé˜²ç½æƒ…å ±XMLãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€ã‚’ç­–å®šãƒ»å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
    - é˜²ç½æƒ…å ±XMLã¯ .XML å½¢å¼ã§æ©Ÿæ¢°å¯èª­ãªæƒ…å ±ãŒæä¾›ã•ã‚Œã€å ±é“æ©Ÿé–¢ãƒ»è‡ªæ²»ä½“ãƒ»é˜²ç½ã‚¢ãƒ—ãƒªç­‰ã§ã®è‡ªå‹•å‡¦ç†ãƒ»é…ä¿¡ã«æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ä¸€èˆ¬äººã®ç§ãŸã¡ã§ã‚‚ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰Pullå‹ã§è‡ªç”±ã«å–å¾—å¯èƒ½ã§ã™ã€‚
    æœ¬ã‚µã‚¤ãƒˆã§ã¯ streamlit community cloud ã®ç·´ç¿’ç”¨ã¨ã—ã¦ã€Atom éšæ™‚ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰ã€Œæ°—è±¡ç‰¹åˆ¥è­¦å ±ãƒ»è­¦å ±ãƒ»æ³¨æ„å ±ã€ã‚’å–å¾—ã—ã¦ .csv(BOMä»˜)ã§å‡ºåŠ›ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚

    - å‚è€ƒ: https://xml.kishou.go.jp/
    
    """)

@st.cache_data(ttl=600)
def fetch_feed(url: str, hours_threshold: int = 48):
    """
    æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰Atomãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã€ãƒªãƒ³ã‚¯ã•ã‚Œã¦ã„ã‚‹XMLãƒ‡ãƒ¼ã‚¿ã‚’ï¼ˆæŒ‡å®šæ™‚é–“å†…ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã®ã¿ï¼‰å–å¾—ã—ã¾ã™ã€‚
    """
    fetched = {"main_feed_xml": None, "linked_entries_xml": []}
    time_threshold = datetime.now(timezone.utc) - timedelta(hours=hours_threshold)

    try:
        resp = requests.get(url, timeout=15)
        resp.raise_for_status()
        fetched["main_feed_xml"] = resp.content

        root = ET.fromstring(fetched["main_feed_xml"].decode("utf-8"))
        atom_ns = "{http://www.w3.org/2005/Atom}"

        for entry in root.findall(f"{atom_ns}entry"):
            entry_info = {
                "EntryID": entry.find(f"{atom_ns}id").text if entry.find(f"{atom_ns}id") is not None else "N/A",
                "FeedReportDateTime": entry.find(f"{atom_ns}updated").text if entry.find(f"{atom_ns}updated") is not None else "N/A",
                "FeedTitle": entry.find(f"{atom_ns}title").text if entry.find(f"{atom_ns}title") is not None else "N/A",
                "Author": entry.find(f"{atom_ns}author/{atom_ns}name").text if entry.find(f"{atom_ns}author/{atom_ns}name") is not None else "N/A",
                "LinkedXMLData": None,
                "LinkedXMLUrl": None
            }

            feed_report_time_str = entry_info.get("FeedReportDateTime")
            skip_by_time = False
            if feed_report_time_str and feed_report_time_str != "N/A":
                try:
                    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ã‚’æ­£ã—ãå‡¦ç†
                    if feed_report_time_str.endswith("Z"):
                        feed_report_time = datetime.fromisoformat(feed_report_time_str[:-1]).replace(tzinfo=timezone.utc)
                    else:
                        feed_report_time = datetime.fromisoformat(feed_report_time_str)
                    
                    # å–å¾—ã—ãŸæ™‚é–“ãŒã—ãã„å€¤ã‚ˆã‚Šå¤ã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if feed_report_time < time_threshold:
                        skip_by_time = True
                except Exception:
                    pass # æ™‚åˆ»ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã—ãªã„ï¼ˆæ¬¡ã®å‡¦ç†ã§è©¦è¡Œï¼‰

            linked_xml_link_element = entry.find(f'{atom_ns}link[@type="application/xml"]')
            # æ™‚é–“ã§ã‚¹ã‚­ãƒƒãƒ—ãƒ•ãƒ©ã‚°ãŒç«‹ã£ã¦ãŠã‚‰ãšã€ãƒªãƒ³ã‚¯è¦ç´ ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿XMLã‚’å–å¾—
            if linked_xml_link_element is not None and not skip_by_time:
                linked_xml_url = linked_xml_link_element.get("href")
                if linked_xml_url:
                    try:
                        lx_resp = requests.get(linked_xml_url, timeout=15)
                        lx_resp.raise_for_status()
                        entry_info["LinkedXMLData"] = lx_resp.content
                        entry_info["LinkedXMLUrl"] = linked_xml_url
                    except Exception as e:
                        entry_info["LinkedXMLData"] = None
                        entry_info["LinkedXMLError"] = str(e)
            fetched["linked_entries_xml"].append(entry_info)

    except Exception as e:
        fetched["error"] = str(e)

    return fetched

def parse_warnings_advisories(fetched_data, hours_threshold: int = 48):
    """
    fetch_feedã‹ã‚‰å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã®ã†ã¡ã€ã€Œæ°—è±¡ç‰¹åˆ¥è­¦å ±ãƒ»è­¦å ±ãƒ»æ³¨æ„å ±ã€ã®ã¿ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¾ã™ã€‚
    """
    parsed = []
    if not fetched_data or not fetched_data.get("linked_entries_xml"):
        return parsed

    time_threshold = datetime.now(timezone.utc) - timedelta(hours=hours_threshold)

    for entry in fetched_data["linked_entries_xml"]:
        feed_title = entry.get("FeedTitle", "N/A")
        # ã€Œæ°—è±¡ç‰¹åˆ¥è­¦å ±ãƒ»è­¦å ±ãƒ»æ³¨æ„å ±ã€ä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—
        if feed_title != "æ°—è±¡ç‰¹åˆ¥è­¦å ±ãƒ»è­¦å ±ãƒ»æ³¨æ„å ±":
            continue

        # fetch_feedã§æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦ã„ã‚‹ãŒã€å¿µã®ãŸã‚ã“ã“ã§ã‚‚ç¢ºèª
        feed_time_str = entry.get("FeedReportDateTime")
        try:
            if feed_time_str and feed_time_str.endswith("Z"):
                feed_time = datetime.fromisoformat(feed_time_str[:-1]).replace(tzinfo=timezone.utc)
            elif feed_time_str:
                feed_time = datetime.fromisoformat(feed_time_str)
            else:
                feed_time = None
        except Exception:
            feed_time = None

        if feed_time and feed_time < time_threshold:
            continue

        extracted = {
            "EntryID": entry.get("EntryID", "N/A"),
            "FeedReportDateTime": entry.get("FeedReportDateTime", "N/A"),
            "FeedTitle": feed_title,
            "Author": entry.get("Author", "N/A"),
            "LinkedXMLDataPresent": bool(entry.get("LinkedXMLData")),
            "LinkedXMLUrl": entry.get("LinkedXMLUrl", "")
        }

        linked_bytes = entry.get("LinkedXMLData")
        warnings = []
        report_dt = extracted["FeedReportDateTime"] # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

        if linked_bytes:
            try:
                xml_text = linked_bytes.decode("utf-8")
            except Exception:
                xml_text = linked_bytes.decode("utf-8", errors="replace") # ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç½®æ›
            try:
                root = ET.fromstring(xml_text)
                # XMLå†…ã®ReportDateTimeã‚’å–å¾—
                rt = root.find('.//{*}ReportDateTime')
                if rt is not None and rt.text:
                    report_dt = rt.text

                # ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ï¼ˆæ¦‚è¦ï¼‰ã‚’å–å¾—
                headline = root.find('.//{*}Headline/{*}Text')
                overall_detail = headline.text if headline is not None and headline.text else "N/A"

                # å„è­¦å ±ãƒ»æ³¨æ„å ±ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ‘ãƒ¼ã‚¹
                items = root.findall('.//{*}Item')
                for item in items:
                    kind_el = item.find('.//{*}Kind/{*}Name')
                    area_el = item.find('.//{*}Areas/{*}Area/{*}Name')
                    # Area/Name ãŒãªã„å ´åˆã€Prefecture/Name ã‚’è©¦ã™
                    if area_el is None:
                        area_el = item.find('.//{*}Areas/{*}Area/{*}Prefecture/{*}Name')

                    kind = kind_el.text if kind_el is not None and kind_el.text else "N/A"
                    area = area_el.text if area_el is not None and area_el.text else "N/A"

                    if kind != "N/A" or area != "N/A":
                        warnings.append({"Kind": kind, "Area": area, "Detail": overall_detail})
            except ET.ParseError:
                warnings.append({"Kind": "è§£æã‚¨ãƒ©ãƒ¼", "Area": "è§£æã‚¨ãƒ©ãƒ¼", "Detail": "XMLè§£æã‚¨ãƒ©ãƒ¼"})
            except Exception:
                warnings.append({"Kind": "ã‚¨ãƒ©ãƒ¼", "Area": "ã‚¨ãƒ©ãƒ¼", "Detail": "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼"})
        else:
            # ãƒªãƒ³ã‚¯ã•ã‚ŒãŸXMLãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆ (fetch_feedã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸå ´åˆãªã©)
            if entry.get("LinkedXMLError"):
                 warnings.append({"Kind": "å–å¾—ã‚¨ãƒ©ãƒ¼", "Area": "å–å¾—ã‚¨ãƒ©ãƒ¼", "Detail": entry.get("LinkedXMLError")})
            elif not extracted["LinkedXMLDataPresent"]:
                 warnings.append({"Kind": "ãƒ‡ãƒ¼ã‚¿ãªã—", "Area": "ãƒ‡ãƒ¼ã‚¿ãªã—", "Detail": "æ™‚é–“å¤–ã¾ãŸã¯å–å¾—å¯¾è±¡å¤–"})
            else:
                 warnings.append({"Kind": "å–å¾—å¤±æ•—", "Area": "å–å¾—å¤±æ•—", "Detail": "ãƒªãƒ³ã‚¯XMLãŒã‚ã‚Šã¾ã›ã‚“"})


        # ãƒ‘ãƒ¼ã‚¹ã—ãŸè­¦å ±æƒ…å ±ãŒã‚ã‚Šã€ã‹ã¤å…ƒãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãŸå ´åˆã®ã¿ãƒªã‚¹ãƒˆã«è¿½åŠ 
        if warnings and extracted["LinkedXMLDataPresent"]:
            extracted["ReportDateTime"] = report_dt # XMLå†…ã®æ—¥æ™‚ã§æ›´æ–°
            extracted["WarningsAdvisories"] = warnings
            parsed.append(extracted)

    return parsed

# --- Streamlit UI ---

st.title("ã€Œæ°—è±¡ç‰¹åˆ¥è­¦å ±ãƒ»è­¦å ±ãƒ»æ³¨æ„å ±ã€ç™ºè¡¨å±¥æ­´æ¤œç´¢ãƒ„ãƒ¼ãƒ« from æ°—è±¡åº é˜²ç½æƒ…å ±XMLï¼ˆé•·æœŸãƒ•ã‚£ãƒ¼ãƒ‰ï¼‰ã‚¢")

col1, col2 = st.columns([1, 2])
with col1:
    st.markdown("### è¨­å®š")
    hours = st.number_input("ä½•æ™‚é–“ä»¥å†…ã®ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã¾ã™ã‹ï¼Ÿ", min_value=1, max_value=168, value=48, step=1)
    if st.button("ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾— / æ›´æ–°"):
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦å†å®Ÿè¡Œ
        st.cache_data.clear()
        st.rerun()

with col2:
    st.markdown("### ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—çŠ¶æ³")
    with st.spinner("ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã¦ã„ã¾ã™..."):
        data = fetch_feed(KISHOU_XML_PAGE_URL, hours_threshold=hours)

if data.get("error"):
    st.error(f"å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {data['error']}")

entries = data.get("linked_entries_xml", [])
st.markdown(f"**ãƒ•ã‚£ãƒ¼ãƒ‰å†…ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ•°**: {len(entries)} ï¼ˆã†ã¡ã€{hours}æ™‚é–“ä»¥å†…ã®XMLå–å¾—å¯¾è±¡ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã®ã¿å‡¦ç†ï¼‰")

# Atom ãƒ•ã‚£ãƒ¼ãƒ‰ã® CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
if entries:
    atom_feed_df = pd.DataFrame(entries)
    # LinkedXMLDataåˆ—ã¯é‡ã„ã®ã§CSVã‹ã‚‰ã¯å‰Šé™¤
    if "LinkedXMLData" in atom_feed_df.columns:
        atom_feed_df = atom_feed_df.drop(columns=["LinkedXMLData"])
        
    csv_buffer_atom = io.StringIO()
    atom_feed_df.to_csv(csv_buffer_atom, index=False, encoding="utf-8-sig")
    st.download_button(
        label="Atom ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ CSV ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv_buffer_atom.getvalue().encode("utf-8-sig"),  # BOMä»˜ãUTF-8
        file_name=f"atom_feed_{datetime.now().strftime('%Y%m%d%H%M%S')}.csv",
        mime="text/csv"
    )

# --- è­¦å ±ãƒ»æ³¨æ„å ±ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç† ---
parsed = parse_warnings_advisories(data, hours_threshold=hours)

if parsed:
    transformed_data_for_db = []
    count_placeholder = st.empty()  # ã‚«ã‚¦ãƒ³ãƒˆã‚¢ãƒƒãƒ—ç”¨ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
    count = 0
    for p in parsed:
        for wa in p.get("WarningsAdvisories", []):
            transformed_data_for_db.append({
                "ReportDateTime": p.get("ReportDateTime"),
                "Title": p.get("FeedTitle"),
                "Author": p.get("Author"),
                "Kind": wa.get("Kind"),
                "Area": wa.get("Area"),
                "Detail": wa.get("Detail"),
                "EntryID": p.get("EntryID")
            })
            count += 1
            count_placeholder.info(f"{count} ä»¶ã®è­¦å ±ãƒ»æ³¨æ„å ±ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")  # åŒã˜æ å†…ã§æ›´æ–°

    csv_buffer_warnings = io.StringIO()
    df = pd.DataFrame(transformed_data_for_db)
    df.to_csv(csv_buffer_warnings, index=False, encoding="utf-8-sig")
    count_placeholder.success(f"{count} ä»¶ã®è­¦å ±ãƒ»æ³¨æ„å ±ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼")  # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    
    st.download_button(
        label="è­¦å ±ãƒ»æ³¨æ„å ±ãƒ‡ãƒ¼ã‚¿ï¼ˆç”Ÿï¼‰ã‚’ CSV ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv_buffer_warnings.getvalue().encode("utf-8-sig"),  # BOMä»˜ãUTF-8
        file_name=f"warnings_raw_{datetime.now().strftime('%Y%m%d%H%M%S')}.csv",
        mime="text/csv"
    )

    # --- â–¼â–¼â–¼ã€æ”¹ä¿®ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰2ã€‘ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã‹ã‚‰è¿½åŠ  â–¼â–¼â–¼ ---
    st.markdown("---") # åŒºåˆ‡ã‚Šç·š
    st.markdown("### ğŸ“Š è­¦å ±ãƒ»æ³¨æ„å ± ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ« (åœ°åŸŸåˆ¥)")

    # dfãŒåˆ©ç”¨å¯èƒ½ã§ç©ºã§ãªã„ã‹ç¢ºèªã—ã¾ã™ã€‚
    if 'df' in locals() and df is not None and not df.empty:
        with st.spinner("ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã¦ã„ã¾ã™..."):
            try:
                # ReportDateTimeã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã—ã€æ—¥ä»˜ã®ã¿ã‚’ä¿æŒã—ã¾ã™ã€‚
                df_pivot = df.copy() # å…ƒã®dfã‚’å¤‰æ›´ã—ãªã„ã‚ˆã†ã«ã‚³ãƒ”ãƒ¼
                df_pivot['ReportDateTime_cleaned'] = pd.to_datetime(df_pivot['ReportDateTime'], errors='coerce')
                df_pivot['ReportDateTime_cleaned'] = df_pivot['ReportDateTime_cleaned'].dt.tz_convert(None) # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å‰Šé™¤
                df_pivot['ReportDate'] = df_pivot['ReportDateTime_cleaned'].dt.date # æ—¥ä»˜ã®ã¿å–å¾—
            except Exception as e:
                st.error(f"ReportDateTimeã®å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
                # å¤‰æ›ã«å¤±æ•—ã—ãŸå ´åˆã€ã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤ºã—ã€ReportDateåˆ—ã¯NaTã§åŸ‹ã‚ã‚‹
                df_pivot['ReportDate'] = pd.NaT 

            # ReportDateãŒæ­£å¸¸ã«ä½œæˆã•ã‚ŒãŸã‹ç¢ºèª (ã™ã¹ã¦NaTã§ãªã„ã‹)
            if 'ReportDate' in df_pivot.columns and not df_pivot['ReportDate'].isnull().all():
                try:
                    # å„è­¦å ±ãƒ»æ³¨æ„å ±ã®ç¨®é¡ï¼ˆKindï¼‰ã¨ReportDate, Title, Authorã®çµ„ã¿åˆã‚ã›ã”ã¨ã«ã€å¯¾è±¡åœ°åŸŸï¼ˆAreaï¼‰ã®ãƒªã‚¹ãƒˆã‚’é›†è¨ˆã—ã¾ã™ã€‚
                    area_kind_summary_df = df_pivot.groupby(['ReportDate', 'Title', 'Author', 'Kind'])['Area'].agg(lambda x: list(x.unique())).reset_index()
                    area_kind_summary_df = area_kind_summary_df.rename(columns={'Area': 'å¯¾è±¡åœ°åŸŸãƒªã‚¹ãƒˆ'})

                    # (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã‚’æŠ˜ã‚ŠãŸãŸã¿ã§è¡¨ç¤º
                    with st.expander("ä¸­é–“ãƒ‡ãƒ¼ã‚¿: ç¨®é¡ã”ã¨ã®å¯¾è±¡åœ°åŸŸãƒªã‚¹ãƒˆï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰"):
                        st.dataframe(area_kind_summary_df)

                    # 'å¯¾è±¡åœ°åŸŸãƒªã‚¹ãƒˆ' åˆ—ã‚’å±•é–‹ã—ã¦ã€ãƒªã‚¹ãƒˆã®å„è¦ç´ ãŒæ–°ã—ã„è¡Œã«ãªã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚
                    expanded_df = area_kind_summary_df.explode('å¯¾è±¡åœ°åŸŸãƒªã‚¹ãƒˆ')

                    # ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚
                    pivot_by_area_kind = pd.pivot_table(
                        expanded_df.fillna({'Kind': 'ä¸æ˜ãªç¨®é¡', 'å¯¾è±¡åœ°åŸŸãƒªã‚¹ãƒˆ': 'ä¸æ˜ãªåœ°åŸŸ'}),
                        values='Kind', # é›†è¨ˆå¯¾è±¡ã¯Kindè‡ªä½“ï¼ˆå­˜åœ¨ã™ã‚Œã°1ï¼‰
                        index=['ReportDate', 'Title', 'Author', 'å¯¾è±¡åœ°åŸŸãƒªã‚¹ãƒˆ'], # å¯¾è±¡åœ°åŸŸã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å«ã‚ã‚‹
                        columns='Kind',
                        aggfunc='size', # å„çµ„ã¿åˆã‚ã›ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆå­˜åœ¨ã™ã‚Œã°1ï¼‰
                        fill_value=0 # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã¾ã™ï¼ˆç™ºä»¤ãªã—ã‚’ç¤ºã™ï¼‰
                    )

                    st.success("å„åœ°åŸŸã”ã¨ã®è­¦å ±/æ³¨æ„å ±ã®ç™ºä»¤çŠ¶æ³ï¼ˆãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰ãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã¾ã—ãŸã€‚")
                    st.dataframe(pivot_by_area_kind) # Streamlitã§DataFrameã‚’è¡¨ç¤º

                    # ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ï¼‰
                    csv_buffer_pivot = io.StringIO()
                    # MultiIndexã‚’ç¶­æŒã—ãŸã¾ã¾CSVã«ä¿å­˜
                    pivot_by_area_kind.to_csv(csv_buffer_pivot, encoding='utf-8-sig') # BOMä»˜ãUTF-8
                    
                    st.download_button(
                        label="åœ°åŸŸåˆ¥ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ CSV ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv_buffer_pivot.getvalue().encode("utf-8-sig"), # BOMä»˜ãUTF-8
                        file_name=f"warnings_pivot_by_area_{datetime.now().strftime('%Y%m%d%H%M%S')}.csv",
                        mime="text/csv"
                    )

                except Exception as e:
                    st.error(f"ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¾ãŸã¯ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
                    pivot_by_area_kind = pd.DataFrame() # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯ç©ºã®DataFrameã‚’ä½œæˆ
            else:
                 st.warning("æ—¥ä»˜æƒ…å ±ã®å¤‰æ›ã«å¤±æ•—ã—ãŸãŸã‚ã€ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    else:
        # ã“ã®åˆ†å²ã¯ 'if parsed:' ã®ä¸­ã«ã‚ã‚‹ã®ã§ã€é€šå¸¸ã¯dfã¯å­˜åœ¨ã™ã‚‹ã¯ãšã ãŒã€å¿µã®ãŸã‚ã€‚
        st.info("ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    # --- â–²â–²â–²ã€æ”¹ä¿®ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰2ã€‘ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã¾ã§è¿½åŠ  â–²â–²â–² ---

else:
    st.info(f"{hours}æ™‚é–“ä»¥å†…ã«ç™ºè¡¨ã•ã‚ŒãŸ 'æ°—è±¡ç‰¹åˆ¥è­¦å ±ãƒ»è­¦å ±ãƒ»æ³¨æ„å ±' ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
