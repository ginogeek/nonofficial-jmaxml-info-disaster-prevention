import streamlit as st
import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
import pandas as pd
import io

KISHOU_XML_PAGE_URL = "https://www.data.jma.go.jp/developer/xml/feed/extra_l.xml"

st.set_page_config(page_title="æ°—è±¡åº é˜²ç½æƒ…å ± (XML) ãƒ“ãƒ¥ãƒ¼ã‚¢", layout="wide")

# --- è¿½åŠ éƒ¨åˆ†: æ°—è±¡åºé˜²ç½æƒ…å ±XMLã®èª¬æ˜ ---
with st.expander("ğŸ“˜ æ°—è±¡åºé˜²ç½æƒ…å ±XMLã¨ã¯ï¼Ÿ", expanded=True):
    st.markdown("""
    æ°—è±¡åºã¯ã€**æ°—è±¡ãƒ»æ´¥æ³¢ãƒ»åœ°éœ‡ãƒ»ç«å±±ãªã©ã®é˜²ç½æƒ…å ±**ã‚’è¿…é€Ÿã‹ã¤æ­£ç¢ºã«ä¼ãˆã‚‹ãŸã‚ã«  
    ã€Œæ°—è±¡åºé˜²ç½æƒ…å ±XMLãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€ã‚’ç­–å®šã—ã€2005å¹´ã‹ã‚‰é‹ç”¨ã—ã¦ã„ã¾ã™ã€‚

    - **ç›®çš„**: è‡ªç„¶ç½å®³ã®è»½æ¸›ã€å›½æ°‘ç”Ÿæ´»ã®å‘ä¸Šã€äº¤é€šå®‰å…¨ã®ç¢ºä¿ã€ç”£æ¥­ã®ç™ºå±•ã‚’æ”¯æ´  
    - **ç‰¹å¾´**:  
        - XMLå½¢å¼ã§æ©Ÿæ¢°å¯èª­ãªé˜²ç½æƒ…å ±ã‚’æä¾›  
        - ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚„è‡ªæ²»ä½“ã‚·ã‚¹ãƒ†ãƒ ãªã©ã§ã®è‡ªå‹•å‡¦ç†ãƒ»é…ä¿¡ãŒå¯èƒ½  
        - ã€ŒPullå‹ã€ã§èª°ã§ã‚‚è‡ªç”±ã«å–å¾—å¯èƒ½ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ç™»éŒ²ä¸è¦ï¼‰  
    - **åˆ©ç”¨ä¾‹**:  
        - é˜²ç½ã‚¢ãƒ—ãƒªã‚„è‡ªæ²»ä½“ã‚·ã‚¹ãƒ†ãƒ ã§ã®è‡ªå‹•é€šçŸ¥  
        - å ±é“æ©Ÿé–¢ã«ã‚ˆã‚‹é€Ÿå ±é…ä¿¡  
        - ç ”ç©¶ãƒ»æ•™è‚²åˆ†é‡ã§ã®ãƒ‡ãƒ¼ã‚¿æ´»ç”¨  

    è©³ç´°ã¯ [æ°—è±¡åºå…¬å¼ã‚µã‚¤ãƒˆ](https://xml.kishou.go.jp/) ã‚’ã”å‚ç…§ãã ã•ã„ã€‚
    """)

@st.cache_data(ttl=600)
def fetch_feed(url: str, hours_threshold: int = 48):
    fetched = {"main_feed_xml": None, "linked_entries_xml": []}
    time_threshold = datetime.now(timezone.utc) - timedelta(hours=hours_threshold)

    try:
        resp = requests.get(url, timeout=15)
        resp.raise_for_status()
        fetched["main_feed_xml"] = resp.content

        root = ET.fromstring(fetched["main_feed_xml"].decode("utf-8"))
        atom_ns = "{http://www.w3.org/2005/Atom}"

        for entry in root.findall(f"{atom_ns}entry"):
            entry_info = {
                "EntryID": entry.find(f"{atom_ns}id").text if entry.find(f"{atom_ns}id") is not None else "N/A",
                "FeedReportDateTime": entry.find(f"{atom_ns}updated").text if entry.find(f"{atom_ns}updated") is not None else "N/A",
                "FeedTitle": entry.find(f"{atom_ns}title").text if entry.find(f"{atom_ns}title") is not None else "N/A",
                "Author": entry.find(f"{atom_ns}author/{atom_ns}name").text if entry.find(f"{atom_ns}author/{atom_ns}name") is not None else "N/A",
                "LinkedXMLData": None,
                "LinkedXMLUrl": None
            }

            feed_report_time_str = entry_info.get("FeedReportDateTime")
            skip_by_time = False
            if feed_report_time_str and feed_report_time_str != "N/A":
                try:
                    if feed_report_time_str.endswith("Z"):
                        feed_report_time = datetime.fromisoformat(feed_report_time_str[:-1]).replace(tzinfo=timezone.utc)
                    else:
                        feed_report_time = datetime.fromisoformat(feed_report_time_str)
                    if feed_report_time < time_threshold:
                        skip_by_time = True
                except Exception:
                    pass

            linked_xml_link_element = entry.find(f'{atom_ns}link[@type="application/xml"]')
            if linked_xml_link_element is not None and not skip_by_time:
                linked_xml_url = linked_xml_link_element.get("href")
                if linked_xml_url:
                    try:
                        lx_resp = requests.get(linked_xml_url, timeout=15)
                        lx_resp.raise_for_status()
                        entry_info["LinkedXMLData"] = lx_resp.content
                        entry_info["LinkedXMLUrl"] = linked_xml_url
                    except Exception as e:
                        entry_info["LinkedXMLData"] = None
                        entry_info["LinkedXMLError"] = str(e)
            fetched["linked_entries_xml"].append(entry_info)

    except Exception as e:
        fetched["error"] = str(e)

    return fetched

def parse_warnings_advisories(fetched_data, hours_threshold: int = 48):
    parsed = []
    if not fetched_data or not fetched_data.get("linked_entries_xml"):
        return parsed

    time_threshold = datetime.now(timezone.utc) - timedelta(hours=hours_threshold)

    for entry in fetched_data["linked_entries_xml"]:
        feed_title = entry.get("FeedTitle", "N/A")
        if feed_title != "æ°—è±¡ç‰¹åˆ¥è­¦å ±ãƒ»è­¦å ±ãƒ»æ³¨æ„å ±":
            continue

        feed_time_str = entry.get("FeedReportDateTime")
        try:
            if feed_time_str and feed_time_str.endswith("Z"):
                feed_time = datetime.fromisoformat(feed_time_str[:-1]).replace(tzinfo=timezone.utc)
            elif feed_time_str:
                feed_time = datetime.fromisoformat(feed_time_str)
            else:
                feed_time = None
        except Exception:
            feed_time = None

        if feed_time and feed_time < time_threshold:
            continue

        extracted = {
            "EntryID": entry.get("EntryID", "N/A"),
            "FeedReportDateTime": entry.get("FeedReportDateTime", "N/A"),
            "FeedTitle": feed_title,
            "Author": entry.get("Author", "N/A"),
            "LinkedXMLDataPresent": bool(entry.get("LinkedXMLData")),
            "LinkedXMLUrl": entry.get("LinkedXMLUrl", "")
        }

        linked_bytes = entry.get("LinkedXMLData")
        warnings = []
        report_dt = extracted["FeedReportDateTime"]

        if linked_bytes:
            try:
                xml_text = linked_bytes.decode("utf-8")
            except Exception:
                xml_text = linked_bytes.decode("utf-8", errors="replace")
            try:
                root = ET.fromstring(xml_text)
                rt = root.find('.//{*}ReportDateTime')
                if rt is not None and rt.text:
                    report_dt = rt.text

                headline = root.find('.//{*}Headline/{*}Text')
                overall_detail = headline.text if headline is not None and headline.text else "N/A"

                items = root.findall('.//{*}Item')
                for item in items:
                    kind_el = item.find('.//{*}Kind/{*}Name')
                    area_el = item.find('.//{*}Areas/{*}Area/{*}Name')
                    if area_el is None:
                        area_el = item.find('.//{*}Areas/{*}Area/{*}Prefecture/{*}Name')

                    kind = kind_el.text if kind_el is not None and kind_el.text else "N/A"
                    area = area_el.text if area_el is not None and area_el.text else "N/A"

                    if kind != "N/A" or area != "N/A":
                        warnings.append({"Kind": kind, "Area": area, "Detail": overall_detail})
            except ET.ParseError:
                warnings.append({"Kind": "è§£æã‚¨ãƒ©ãƒ¼", "Area": "è§£æã‚¨ãƒ©ãƒ¼", "Detail": "XMLè§£æã‚¨ãƒ©ãƒ¼"})
            except Exception:
                warnings.append({"Kind": "ã‚¨ãƒ©ãƒ¼", "Area": "ã‚¨ãƒ©ãƒ¼", "Detail": "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼"})
        else:
            warnings.append({"Kind": "å–å¾—å¤±æ•—", "Area": "å–å¾—å¤±æ•—", "Detail": "ãƒªãƒ³ã‚¯XMLãŒã‚ã‚Šã¾ã›ã‚“"})

        if warnings and extracted["LinkedXMLDataPresent"]:
            extracted["ReportDateTime"] = report_dt
            extracted["WarningsAdvisories
